{
  "metadata": {
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "authors": [
      "Qizheng Zhang",
      "Changran Hu",
      "Shubhangi Upasani",
      "Boyuan Ma",
      "Fenglu Hong",
      "Vamsidhar Kamanuru",
      "Jay Rainton",
      "Chen Wu",
      "Mengmeng Ji",
      "Hanchen Li",
      "Urmish Thakker",
      "James Zou",
      "Kunle Olukotun"
    ],
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation—modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "keywords": null,
    "field_of_study": "Artificial Intelligence",
    "paper_type": "research"
  },
  "executive_summary": {
    "summary": "This paper introduces Agentic Context Engineering (ACE), a novel framework for continuously evolving and refining LLM contexts. ACE addresses critical limitations of existing methods, such as 'brevity bias' and 'context collapse,' by treating contexts as dynamic playbooks that accumulate and organize strategies through a modular generation, reflection, and curation process. The framework demonstrates significant performance improvements across agent and domain-specific benchmarks, achieving higher accuracy and substantial reductions in adaptation latency and cost, even when using smaller open-source models. Its ability to self-improve without labeled supervision makes it a highly promising approach for scalable and efficient LLM systems."
  },
  "methodology": {
    "rating": "excellent",
    "soundness": "The methodological approach is logically sound, building on agentic architectures (Dynamic Cheatsheet [41]) and explicitly addressing the identified limitations of brevity bias and context collapse (Section 2.2). The modular workflow of Generator, Reflector, and Curator (Section 3, Figure 4) provides a structured and coherent approach to continuous context adaptation.",
    "experimental_design": "The experimental design is robust, evaluating ACE on two distinct categories of LLM applications: agents (AppWorld benchmark) and domain-specific benchmarks (FINER, Formula for financial analysis) (Section 4.1). Comparisons are made against strong and relevant baselines (ICL, MIPROv2, GEPA, DC) (Section 4.2). Ablation studies (Section 4.5, Table 3) effectively isolate and demonstrate the contribution of individual ACE components.",
    "statistical_rigor": "The paper reports clear average gains and specific percentage improvements (e.g., +10.6% on agents, 8.6% on finance in abstract and Section 4). Tables 1, 2, and 3 include standard deviations/ranges, indicating an assessment of variability. The evaluation metrics (TGC, SGC, Accuracy) are standard and appropriately applied for the respective benchmarks (Section 4.1).",
    "reproducibility": "The paper enhances reproducibility by stating that \"We release the language model prompts used in our agentic context engineering framework as well as the baselines to support research transparency and reproducibility\" (Appendix D). It also references the official implementations for all baselines (DSPy implementations for MIPROv2 [15] and GEPA [14], Dynamic Cheatsheet [42]). The use of an open-source model (DeepSeek-V3.1) further aids in replication.",
    "strengths": [
      "The modular architecture of Generator, Reflector, and Curator (Section 3, Figure 4) provides a clear and effective division of labor for adaptive context engineering, allowing for specialized roles in strategy generation, insight extraction, and structured updates.",
      "The incremental delta updates and grow-and-refine mechanism (Section 3.1, 3.2) effectively mitigate 'context collapse' and 'brevity bias', preserving detailed knowledge and enabling scalable context expansion, which is a significant advancement over prior monolithic rewriting approaches (Figure 2).",
      "ACE facilitates self-improvement without requiring labeled supervision by leveraging natural execution feedback and environment signals, making it highly practical for real-world agentic systems (Section 4.3).",
      "Comprehensive ablation studies (Section 4.5, Table 3) clearly demonstrate the positive impact of specific design choices, such as the Reflector with iterative refinement and multi-epoch adaptation, on the overall performance of the framework."
    ],
    "weaknesses": [
      "The paper acknowledges a potential limitation regarding ACE's reliance on a 'reasonably strong Reflector' and notes that performance may degrade without 'reliable feedback signals' (Appendix B, Section 4.4), which could limit its applicability in certain low-feedback environments.",
      "While baselines reference open-source implementations, a direct and prominent link to the ACE framework's own implementation is not provided in the main text, potentially hindering immediate access and reproducibility for researchers."
    ]
  },
  "novelty": {
    "rating": "excellent",
    "originality": "The core idea of treating contexts as \"evolving playbooks\" through a modular generation-reflection-curation process (Section 3) is highly original. This approach directly addresses critical and under-explored problems in context adaptation, namely \"brevity bias\" and \"context collapse\" (Section 2.2), offering a unique solution to known limitations.",
    "significance": "ACE holds high potential impact due to its ability to enable self-improving LLM systems with enhanced accuracy, reduced computational cost, and lower latency (Section 4.6, Table 4) even with smaller models (Abstract, Section 4.3). Its modularity and independence from labeled supervision are crucial for advancing future research in online and continuous learning, marking it as a significant contribution to the field.",
    "incremental_vs_breakthrough": "This is a breakthrough contribution. While it builds upon existing agentic architectures like Dynamic Cheatsheet [41], ACE's paradigm shift in structured, incremental context evolution and its explicit solution to context collapse fundamentally redefine how LLM contexts are managed, moving beyond simple prompt optimization or monolithic rewriting.",
    "comparison_to_prior_work": "The paper provides a clear and detailed comparison of ACE with prior works such as Dynamic Cheatsheet [41], Reflexion [40], TextGrad [54], and GEPA [4] in Sections 2.1 and 2.2. It meticulously explains how ACE surpasses these methods by preventing \"brevity bias\" and \"context collapse\" through its mechanism of accumulating and refining detailed playbooks rather than compressing knowledge. Quantitative comparisons in Section 4.3 and 4.4 demonstrate ACE's superior performance over these baselines."
  },
  "writing": {
    "rating": "good",
    "clarity": "The paper is generally clear and well-written. Key concepts such as 'brevity bias' and 'context collapse' are clearly defined and motivated in Section 2.2. Technical terms are appropriately used and implicitly understood within the domain. The overall flow is logical, making it relatively easy to follow the progression from problem identification to the proposed solution.",
    "organization": "The paper exhibits a logical structure, starting with the abstract and introduction, transitioning to a discussion of prior work's limitations, presenting the ACE framework, detailing experimental results, and concluding with a discussion of implications. Subsections are used effectively to segment content. However, some transitions between paragraphs could be smoother for a more fluid reading experience.",
    "grammar_and_style": "The grammar is largely correct throughout the paper, and the writing style maintains a professional and academic tone. There are no significant grammatical errors or stylistic inconsistencies that detract from readability. The language is precise and appropriate for a scientific publication.",
    "suggestions": [
      "Consider adding a concise, high-level diagram illustrating the 'grow-and-refine' mechanism in Section 3.2 to provide a better visual aid for understanding this key process.",
      "While generally clear, some sentences are quite long and complex; breaking them down into shorter, more direct sentences could enhance overall readability and comprehension.",
      "Ensure that all acronyms (e.g., LLM) are explicitly defined at their first appearance in the main text, even if they are widely recognized in the field, to maintain full accessibility for all readers."
    ]
  },
  "visual_elements": {
    "overall_rating": "excellent",
    "tables": {
      "rating": "excellent",
      "clarity": "Tables 1, 2, 3, and 4 are exceptionally clear. Headers such as \"Method\", \"GT Labels\", \"Test-Normal\", \"Test-Challenge\", \"Average\", \"Latency (s)\", \"# Rollouts↓\", and \"Token Cost ($)↓\" are well-defined. Column and row labels are unambiguous and easy to understand.",
      "data_organization": "The data in all tables is logically organized, with results grouped appropriately by benchmark, adaptation setting (offline vs. online), and specific comparisons (e.g., ablation studies). The organization scheme is consistent across all presented tables, facilitating easy comparison.",
      "formatting_consistency": "All tables exhibit excellent formatting consistency. Font sizes, alignment, and general presentation adhere to a professional standard throughout the paper, which contributes significantly to their readability and aesthetic quality.",
      "completeness": "All necessary information is present in the tables, including units (seconds for latency, $ for token cost), and statistical variations (e.g., +12.5) where relevant. The meaning of \"GT Labels\" is clearly explained in the captions, ensuring full understanding.",
      "relevance": "All tables are highly relevant, effectively presenting comparative performance results, ablation study findings, and cost/speed analyses. They directly support the paper's claims and are consistently referred to and discussed in the main text (e.g., Section 4, Section 4.5, Section 4.6).",
      "issues": [],
      "suggestions": []
    },
    "figures": {
      "rating": "good",
      "visual_design": "Figure 1 (bar chart) and Figure 2 (line plot) are aesthetically clean and uncluttered. Colors are distinguishable and chosen appropriately. Figure 4 (workflow diagram) is clearly laid out, visually guiding the reader through the ACE framework. Figures 6-14, which illustrate prompts, are also very clear.",
      "clarity_readability": "Axis labels (e.g., \"Accuracy (%)\", \"Tokens in context\", \"# adaptation steps\") are clear and sufficiently large for readability. All text and annotations within figures are legible, and font sizes are consistent. Colors in bar charts and line plots are distinct, supporting visual differentiation.",
      "data_accuracy": "Figure 1 provides numerical values on top of the bars, enhancing data accuracy. Figure 2 accurately depicts token counts and accuracy over adaptation steps. Axes are labeled with appropriate units. Legends are complete and clearly define all symbols or colors used.",
      "chart_type_appropriateness": "The chosen visualization formats—bar charts for comparative performance, line plots for trends over time, and diagrams for illustrating workflows—are highly appropriate for the data and concepts they present, effectively conveying information and enabling clear identification of trends and comparisons.",
      "caption_quality": "Captions for all figures are descriptive and complete. For example, Figure 1's caption clearly states its purpose as illustrating overall performance, and Figure 2's effectively explains the phenomenon of context collapse. Captions for Figures 6-14 also clearly describe the prompts.",
      "issues": [
        "In Figure 1, the y-axis for 'Domain Knowledge: FINER' and 'Numerical Reasoning: Formula' is labeled 'Accuracy (%)' but uses different scales (e.g., starting at 66 for Formula vs. 40 for Agent: AppWorld). This could cause initial confusion without careful inspection of individual subplot scales."
      ],
      "suggestions": [
        "In Figure 1, to enhance clarity, explicitly add individual y-axis labels to each subplot or clearly state in the caption that the y-axis scale varies across the different tasks displayed."
      ]
    },
    "diagrams": {
      "rating": "excellent",
      "structure_clarity": "Figure 4, illustrating 'The ACE Framework', clearly presents the modular architecture of the Generator, Reflector, and Curator components. The overall structure is well-organized, and the layout logically represents the iterative refinement process of context adaptation.",
      "component_labeling": "All components in Figure 4 are clearly labeled (e.g., 'LLM Generator', 'LLM Reflector', 'LLM Curator', 'Context Playbook', 'Delta Context Items'). Labels are concise yet descriptive, aiding immediate understanding of each element's role.",
      "flow_connections": "Connections and arrows between components in Figure 4 are clear and accurately represent the flow from 'Query' to 'Trajectory', 'Insights', 'Update', and 'Delta Context Items'. The flow direction is intuitive, effectively guiding the reader through the framework's operations.",
      "annotation_quality": "The diagram in Figure 4 is clean and free of clutter. Annotations are helpful, such as 'Iterative Refinement', and are visually well-integrated without distracting from the core structure and flow.",
      "alignment_with_text": "Figure 4 is directly referenced and explained comprehensively in Section 3, titled 'Agentic Context Engineering (ACE)'. The textual explanation aligns perfectly with the diagram, enabling a complete understanding of the framework's design and operation.",
      "issues": [],
      "suggestions": []
    },
    "images": null,
    "visual_consistency": "The visual elements across the paper, including tables, figures, and diagrams, exhibit high consistency in terms of style, color schemes, font sizes, axis formatting, and notation conventions. All elements are consistent with the paper's overall aesthetic and follow established field conventions, enhancing the professional presentation.",
    "caption_quality": "The quality of all captions and legends is excellent. They are consistently complete, clear, and highly informative, allowing readers to understand the visuals without extensive reference to the main text. Legends clearly define all symbols or colors, and units are specified where necessary, ensuring accurate interpretation of the data."
  },
  "literature": {
    "rating": "excellent",
    "comprehensiveness": "The paper provides a comprehensive review of relevant literature, citing key foundational work in context adaptation (Reflexion [40], TextGrad [54], GEPA [4], Dynamic Cheatsheet [41] in Section 2.1) and related areas like long-context LLMs, agent memory, and prompt optimization. Important recent and classical references are included, establishing a solid contextual foundation.",
    "gaps_identified": [],
    "citation_accuracy": "Citations are accurately attributed and appropriately contextualized throughout the paper. Authors correctly explain how their work builds upon or differs from prior research, providing a balanced and fair representation of the existing literature. No instances of misrepresentation or selective reporting were identified."
  },
  "results": {
    "rating": "excellent",
    "result_quality": "The results presented in Section 4 and Tables 1-4 are of high quality, clearly demonstrating significant and consistent performance gains of ACE over strong baselines across multiple benchmarks (AppWorld, FINER, Formula). The quantitative improvements (e.g., +10.6% on agents, +8.6% on finance in the abstract, detailed percentages in Section 4.3 and 4.4) are substantial, and the cost and latency reductions are well-documented (Table 4).",
    "interpretation": "The interpretations of the results are sound and logically follow from the data presented. The paper effectively explains *why* ACE outperforms baselines by linking back to its core design principles, such as structured contexts and incremental updates, which prevent context collapse (Section 4.3, 4.4). The authors also provide a cautious discussion of limitations, such as the dependence on reliable feedback signals (Section 4.4, Appendix B).",
    "limitations_assessment": "The authors thoroughly discuss the limitations of ACE, acknowledging its reliance on a strong Reflector and the potential for performance degradation if reliable feedback signals are absent (Appendix B, Section 4.4). They also effectively contextualize when ACE is most beneficial (e.g., demanding detailed domain knowledge, complex tool use, Appendix B), avoiding overgeneralization and presenting a realistic scope.",
    "limitations_discussed": true
  },
  "ethical_considerations": null,
  "overall_assessment": {
    "strengths": [
      "ACE effectively addresses and provides a robust solution to the critical problems of 'brevity bias' and 'context collapse' in LLM context adaptation, which are significant limitations of existing methods.",
      "The framework achieves substantial performance improvements on complex agentic and domain-specific reasoning tasks, consistently outperforming strong baselines by significant margins as evidenced in Tables 1 and 2.",
      "ACE demonstrates remarkable efficiency, drastically reducing adaptation latency and token costs compared to existing adaptive methods, making it a highly practical and scalable solution (Table 4).",
      "Its ability to self-improve and adapt effectively without relying on labeled supervision, leveraging natural execution feedback and environment signals, is a major advantage for developing autonomous and self-optimizing AI systems.",
      "The modular architecture of the Generator, Reflector, and Curator is well-designed, providing a clear division of labor, and its efficacy is strongly supported by comprehensive ablation studies (Table 3)."
    ],
    "weaknesses": [
      "The effectiveness of ACE is somewhat contingent on the quality of the Reflector and the availability of reliable feedback signals; performance could degrade in low-feedback or noisy environments, as discussed in Appendix B.",
      "Although prompts are made available in the appendix, a direct and prominent link to the full source code repository for the ACE implementation is not provided in the main paper, which could slightly impede immediate reproducibility and adoption.",
      "The paper primarily evaluates ACE using DeepSeek-V3.1 as the base LLM; a broader exploration of its performance and generalizability with a wider range of diverse foundation models would further strengthen the claims."
    ]
  },
  "introduction_quality": {
    "focus_rating": "excellent",
    "unnecessary_content": false,
    "relevance_analysis": "The introduction efficiently sets the stage by discussing the increasing reliance of modern AI applications on context adaptation. It then clearly and effectively identifies the key limitations of existing approaches—brevity bias and context collapse—which ACE is designed to address. All background material is directly relevant and logically builds the motivation for the proposed research.",
    "issues": []
  },
  "claims_accuracy": {
    "over_claiming": false,
    "evidence_support": "excellent",
    "factual_accuracy": "All numerical claims, such as the \"+10.6% on agents and +8.6% on finance\" improvement reported in the abstract, are factually accurate and directly substantiated by the detailed quantitative results presented in Tables 1, 2, and Figure 1. The comparisons to baselines and the reported cost/latency reductions are also accurately backed by data in Tables 3 and 4.",
    "claim_strength": [
      "Well-supported: 'ACE consistently outperforms strong baselines, yielding average gains of 10.6% on agents and 8.6% on domain-specific benchmarks, across both offline and online adaptation settings' (Abstract, Section 4.3, 4.4, Tables 1, 2).",
      "Well-supported: 'ACE achieves 86.9% lower adaptation latency (on average) than existing adaptive methods, demonstrating that scalable self-improvement can be achieved with both higher accuracy and lower overhead' (Section 4.6, Table 4).",
      "Well-supported: 'On the AppWorld benchmark leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model' (Abstract, Section 4.3, Figure 5)."
    ]
  },
  "ai_generation": {
    "is_likely_ai_generated": false,
    "authenticity_score": "authentic",
    "analysis": "The paper exhibits clear and original technical insights, presenting a sophisticated argumentation that critically analyzes existing limitations in context adaptation and proposes a well-structured, novel solution. The writing maintains a natural, fluent voice, avoids repetitive phrasing or generic transitions, and consistently demonstrates deep domain expertise, all of which are strong indicators of human authorship. The specific problem formulation and detailed architectural design reflect genuine scientific ingenuity.",
    "indicators": [
      "Natural voice and novel insights",
      "Sophisticated argumentation",
      "Clear and specific problem formulation and solution"
    ],
    "confidence": "high"
  },
  "specific_issues": {
    "major_issues": [],
    "minor_issues": [
      "Figure 1: The y-axis scales for the subplots 'Domain Knowledge: FINER' and 'Numerical Reasoning: Formula' differ from 'Agent: AppWorld'. While self-contained, adding explicit y-axis labels for each subplot or clarifying this scale difference in the caption would improve immediate clarity."
    ],
    "questions_for_authors": [
      "Could the authors elaborate on the potential computational cost and latency benefits of ACE if a stronger, proprietary LLM (e.g., GPT-4.1) were consistently used for the Reflector and Curator, instead of DeepSeek-V3.1, particularly in scenarios where the quality of feedback might otherwise be challenging?",
      "How does the 'grow-and-refine' mechanism explicitly handle potential conflicts or contradictions that might arise when combining insights from different Generator trajectories, especially if the Reflector or Curator makes suboptimal or inconsistent judgments over time?"
    ]
  },
  "detailed_feedback": {
    "comments_for_authors": "This paper presents a highly innovative and impactful framework, Agentic Context Engineering (ACE), which effectively addresses significant limitations in current LLM context adaptation methodologies, specifically 'brevity bias' and 'context collapse.' The modular architecture, comprising Generator, Reflector, and Curator components, is well-conceived and offers a scalable solution for evolving contexts as 'playbooks.' The empirical results are compelling, demonstrating substantial performance improvements on both agentic and domain-specific benchmarks, alongside impressive reductions in adaptation latency and cost. The ability of ACE to operate and self-improve without labeled supervision, utilizing execution feedback, is a major step forward for robust AI systems. The ablation studies clearly validate the contribution of individual design choices. To further enhance the paper, I recommend explicitly addressing the minor issues identified. Clarifying the differing y-axis scales in Figure 1 would improve visual interpretation. Additionally, while prompts are in the appendix, a direct link to the ACE framework's code repository in the main text would greatly facilitate reproducibility and wider adoption by the community. These are minor points in an otherwise excellent and thoroughly conducted piece of research.",
    "confidential_comments_to_editor": null
  },
  "recommendation": {
    "decision": "accept",
    "confidence": "high",
    "justification": "The paper introduces a novel and highly effective framework, ACE, that rigorously addresses crucial limitations of existing LLM context adaptation methods. It demonstrates significant performance gains and efficiency improvements across diverse, challenging benchmarks. The work is well-supported by robust experimental design and clear analysis. Its strengths, particularly the innovative architectural design and the ability to self-improve without labeled supervision, far outweigh the minor presentation and reproducibility-related weaknesses, warranting acceptance for publication.",
    "conditions_for_acceptance": [
      "Please address the minor visual clarity issue in Figure 1 regarding the differing y-axis scales in subplots, either by adding explicit labels for each or clarifying this in the caption.",
      "Please provide a direct and prominent link to the ACE framework's implementation (e.g., a GitHub repository) in the main paper (e.g., in Section 3 or the Abstract) to improve accessibility and reproducibility."
    ]
  }
}