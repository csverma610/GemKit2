{
  "metadata": {
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "authors": [
      "Qizheng Zhang",
      "Changran Hu",
      "Shubhangi Upasani",
      "Boyuan Ma",
      "Fenglu Hong",
      "Vamsidhar Kamanuru",
      "Jay Rainton",
      "Chen Wu",
      "Mengmeng Ji",
      "Hanchen Li",
      "Urmish Thakker",
      "James Zou",
      "Kunle Olukotun"
    ],
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation—modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "keywords": [
      "LLM agents",
      "context adaptation",
      "self-improving LLMs",
      "dynamic cheatsheet",
      "context engineering"
    ],
    "field_of_study": "Machine Learning",
    "paper_type": "research"
  },
  "executive_summary": {
    "summary": "This paper introduces Agentic Context Engineering (ACE), a novel framework designed to enhance the self-improvement capabilities of Large Language Models (LLMs) through dynamic context adaptation. ACE addresses critical limitations of existing methods, namely 'brevity bias' and 'context collapse,' by treating contexts as evolving, structured playbooks. It achieves state-of-the-art performance on agent and domain-specific benchmarks, such as AppWorld and financial analysis tasks (FINER, Formula), demonstrating average gains of 10.6% and 8.6% respectively over strong baselines. The framework is notable for its efficiency, significantly reducing adaptation latency and costs, and its ability to adapt without explicit labeled supervision by leveraging natural execution feedback."
  },
  "methodology": {
    "rating": "excellent",
    "soundness": "The methodology is theoretically sound, building upon the concept of adaptive memory from Dynamic Cheatsheet [41] but extending it to address specific limitations. The three-component agentic architecture (Generator, Reflector, Curator) described in Section 3 and Figure 4 provides a clear and logical division of labor, enhancing robustness and scalability.",
    "experimental_design": "The experimental design is comprehensive, evaluating ACE across two distinct categories of LLM applications: multi-turn reasoning agents (AppWorld) and domain-specific benchmarks (FINER, Formula) as detailed in Section 4.1. The use of both offline and online adaptation settings provides a thorough assessment of the framework's versatility. Baselines, including ICL, MIPROv2, GEPA, and Dynamic Cheatsheet, are appropriately chosen and implemented fairly (Section 4.2).",
    "statistical_rigor": "The paper reports average performance gains and uses standard metrics like Task Goal Completion (TGC) and Scenario Goal Completion (SGC) for AppWorld, and accuracy for financial tasks. While p-values or confidence intervals for the performance differences are not explicitly given in the text beyond the presented average gains and standard deviation in the tables, the consistency of improvements across multiple benchmarks suggests statistical significance.",
    "reproducibility": "The authors provide details on the base LLM (DeepSeek-V3.1), baselines' implementations (DSPy, official releases), and specific settings for ACE (batch size, refinement rounds, epochs) in Section 4.2. Prompt examples for ICL, DC, GEPA, and ACE are included in Appendix D (Figures 6-9, 11-14). This level of detail, along with citing official implementations and open-source models, suggests a strong commitment to reproducibility.",
    "strengths": [
      "The modular agentic architecture with Generator, Reflector, and Curator roles significantly improves context quality and scalability by distributing responsibilities.",
      "The incremental delta updates (Section 3.1) and grow-and-refine mechanism (Section 3.2) effectively prevent context collapse and brevity bias, which are key limitations of prior work.",
      "ACE can adapt effectively without labeled supervision, leveraging execution feedback, making it suitable for self-improving systems (Section 4.3)."
    ],
    "weaknesses": [
      "The paper mentions in Section 4.4 and Appendix B that ACE's effectiveness depends critically on the quality of feedback signals; without reliable ground-truth supervision or execution outcomes, performance may degrade. This dependency highlights a practical limitation.",
      "Ablation studies (Table 3) show the contribution of individual components, but further analysis on the interaction effects of these components could provide deeper insights."
    ]
  },
  "novelty": {
    "rating": "excellent",
    "originality": "ACE introduces a highly original approach to context adaptation by conceptualizing contexts as 'evolving playbooks' rather than static prompts or distilled summaries. The modular framework (Generator, Reflector, Curator) and the principles of incremental delta updates and grow-and-refine are novel contributions that directly address the significant problems of brevity bias and context collapse (Section 2.2).",
    "significance": "The contribution is highly significant, offering a scalable, efficient, and self-improving paradigm for LLM systems, especially for agents and domain-specific reasoning (Section 1, Section 5). By enabling LLMs to learn and refine strategies continuously with low overhead, ACE paves the way for more robust and capable AI applications, influencing future research in dynamic context management and online learning.",
    "incremental_vs_breakthrough": "This work represents a moderate to breakthrough advance. While it builds on adaptive memory concepts like Dynamic Cheatsheet [41], its structured, modular, and incremental approach to context evolution, explicitly designed to overcome pervasive limitations (brevity bias, context collapse), signifies a substantial leap forward. The ability to achieve top-tier performance with smaller open-source models (Section 4.3) further underscores its breakthrough potential.",
    "comparison_to_prior_work": "The paper thoroughly compares ACE to prior context adaptation methods, explicitly highlighting their limitations. It references GEPA [4] for brevity bias and Dynamic Cheatsheet [41] for context collapse (Section 2.2 and Figure 2). ACE distinguishes itself by preventing these issues through structured, incremental updates. The paper clearly articulates how ACE improves upon ICL [3], MIPROv2 [36], and GEPA by providing comprehensive, evolving contexts rather than short, generic prompts or monolithic rewrites, enabling higher accuracy and lower costs as shown in Tables 1 and 4."
  },
  "writing": {
    "rating": "excellent",
    "clarity": "The writing is exceptionally clear and concise. Complex concepts like 'brevity bias' and 'context collapse' are well-defined and illustrated (Figure 2, Section 2.2). Technical terms are introduced and explained effectively. The abstract, introduction, and methodology sections are particularly well-written, guiding the reader through the problem, proposed solution, and evaluation with ease.",
    "organization": "The paper is well-structured and logically organized. It begins with a clear introduction, precisely defines the problem and limitations of existing work, presents the ACE framework in detail, and then systematically evaluates its performance. Sections flow coherently, and the use of headings and subheadings facilitates navigation. The appendices provide useful prompt examples, enhancing transparency.",
    "grammar_and_style": "The grammar and writing style are excellent. The language is professional, precise, and consistent throughout the paper. There are no noticeable grammatical errors or awkward sentence constructions. The tone is academic and objective, making the paper highly readable.",
    "suggestions": [
      "The paper could benefit from a short table summarizing the key differences between ACE and the most closely related work (e.g., Dynamic Cheatsheet, GEPA) in terms of features, rather than just limitations, to highlight differentiators concisely.",
      "Consider briefly mentioning potential negative societal impacts or ethical considerations if the context adaptation leads to propagating harmful biases from observed behaviors."
    ]
  },
  "visual_elements": {
    "rating": "excellent",
    "tables_quality": "Tables 1, 2, 3, and 4 are exceptionally clear, well-organized, and effectively summarize the experimental results. Headers are well-defined, and footnotes (`GT labels`, `Acc↑`, `SGC↑`, `TGC↑`) provide necessary context. The consistent use of `DeepSeek-V3.1 as Base LLM` in all tables helps standardize comparisons. The delta values (e.g., `+10.6%`) clearly show improvements over baselines, making the tables highly informative.",
    "figures_quality": "Figures 1, 2, 3, and 4 are high-quality and directly support the text. Figure 1 clearly shows performance improvements across different tasks, with well-labeled axes and distinct bars for each method. Figure 2 vividly illustrates 'context collapse' with accuracy and token count over adaptation steps. Figure 4 provides an excellent high-level overview of the ACE framework components and their interaction, crucial for understanding the methodology.",
    "diagrams_and_flowcharts": "Figure 4 (The ACE Framework) serves as a clear architectural diagram, effectively illustrating the workflow between Generator, Reflector, and Curator components. The arrows show data flow and iterative refinement. Figure 3 (Example ACE-Generated Context) is an effective visual representation of the 'playbook' structure, showing structured insights, code snippets, and troubleshooting advice, which are well-annotated.",
    "images_quality": "The paper does not rely on photographs or complex visualizations beyond standard plots and diagrams, which are all of high quality and relevant to the content. The resolution of all figures and tables is excellent, with legible text and labels.",
    "visual_consistency": "There is strong visual consistency across all figures and tables. Font styles, color schemes (where applicable), and layout are uniform, contributing to a professional presentation. The use of bars and lines in plots is consistent, and table formatting is uniform.",
    "caption_quality": "Captions for Figures 1, 2, 3, 4, and Tables 1, 2, 3, 4 are comprehensive, clear, and informative. They accurately describe the content of the visual elements and often include key takeaways or explanations, making the figures and tables largely self-contained. For instance, Figure 3's caption explains the contents of the example playbook.",
    "data_presentation": "Data presentation is highly effective. Figure 1 uses bar charts to compare performance across methods, clearly highlighting ACE's improvements. Figure 2's line plots with two Y-axes effectively demonstrate the correlation between token count and accuracy during context collapse. Tables use clear numerical values and explicit deltas to show performance differences, making comparisons straightforward and impactful. Error bars in tables 1, 2 and 3 are also present and appropriately used.",
    "visual_issues": [
      "No significant visual issues were identified. All figures and tables are well-produced and enhance the understanding of the paper."
    ],
    "visual_suggestions": [
      "Consider adding a small legend or color key to Figure 1 and Figure 2 if different shades have a specific meaning, although current distinctions are clear enough.",
      "For Figure 3, a more explicit visual indicator could be added to highlight where specific strategies, code, or pitfalls are integrated into the playbook structure."
    ]
  },
  "literature": {
    "rating": "excellent",
    "comprehensiveness": "The literature review is comprehensive, covering foundational work in context adaptation (Section 2.1), limitations of existing methods (Section 2.2), and related work on agent memory (Section A, page 14). It cites key papers in LLM agents, prompt optimization, long-context LLMs, and transfer learning, demonstrating a thorough understanding of the field. Both recent and classical references are included.",
    "gaps_identified": [
      "While a broad range of related work is cited, a more explicit discussion on how ACE relates to recent advancements in automated prompt engineering beyond GEPA and MIPROv2, particularly those focused on multi-modal contexts, could be valuable, if applicable.",
      "Given the emphasis on self-improvement, a brief acknowledgment of work in continual learning or lifelong learning specifically for LLMs that goes beyond context adaptation could provide broader context, though the paper's focus is on context adaptation."
    ],
    "citation_accuracy": "Citations are consistently accurate and appropriately contextualized throughout the paper. References are used to support claims, describe prior work, and provide context for the proposed methodology. The reference list itself is well-formatted and comprehensive, covering a wide range of relevant publications."
  },
  "results": {
    "rating": "excellent",
    "result_quality": "The results are of high quality and well-supported by experimental data presented in Tables 1, 2, 3, and 4. Performance gains are consistently shown across diverse benchmarks (AppWorld, FINER, Formula) and adaptation settings (offline, online). The ablation studies (Table 3) effectively validate the design choices, and the cost/speed analysis (Table 4) provides strong evidence for ACE's efficiency. Error bars are included where appropriate, enhancing result validity.",
    "interpretation": "The interpretation of results is sound and avoids over-claiming. The authors carefully explain why ACE outperforms baselines (e.g., addressing brevity bias and context collapse) and provide insights into its robustness (e.g., adaptation without labeled supervision). The discussion on limitations (Section 4.4, Appendix B) regarding feedback quality is balanced and realistic, acknowledging the conditions under which ACE performs best.",
    "limitations_assessment": "The paper includes a dedicated section on 'Limitations and Challenges' (Appendix B), which thoroughly assesses the framework's dependencies on a strong Reflector and the quality of feedback signals. It also clarifies that ACE is most beneficial for tasks demanding detailed domain knowledge or complex tool use, not all applications. This self-critical assessment enhances the credibility of the work.",
    "limitations_discussed": true
  },
  "ethical_considerations": null,
  "overall_assessment": {
    "strengths": [
      "ACE effectively addresses critical limitations of existing context adaptation methods, specifically 'brevity bias' and 'context collapse', through its novel evolving playbook paradigm.",
      "The modular architecture (Generator, Reflector, Curator) with incremental updates leads to significant performance improvements across diverse benchmarks (agentic tasks and financial analysis) while maintaining scalability.",
      "ACE demonstrates remarkable efficiency, achieving substantial reductions in adaptation latency and computational costs compared to strong baselines, making it practical for real-world deployment.",
      "The framework exhibits strong self-improvement capabilities by effectively leveraging natural execution feedback even without explicit ground-truth labels."
    ],
    "weaknesses": [
      "The effectiveness of ACE is critically dependent on the quality of feedback signals (e.g., ground-truth labels or execution outcomes), which may limit its applicability in scenarios with poor feedback quality.",
      "While the paper demonstrates strong performance, the underlying DeepSeek-V3.1 model might still be less powerful than proprietary models like GPT-4.1, which could affect its absolute performance ceiling in some extremely complex tasks.",
      "The paper focuses on the technical aspects of context adaptation; a brief discussion on potential biases that could be introduced or amplified through context evolution, and how ACE mitigates them, would be valuable."
    ]
  },
  "specific_issues": {
    "major_issues": [
      "Issue: The paper states that ACE can adapt effectively without labeled supervision, but then mentions its dependence on 'reliable feedback signals' in Section 4.4 and Appendix B. Impact: This creates a slight ambiguity on the true autonomy and robustness of the framework without any form of external guidance."
    ],
    "minor_issues": [
      "Figure 2's y-axis labels could be slightly clearer if 'Accuracy (%)' and '# Tokens' were explicitly on their respective sides, though it is understandable as presented.",
      "The discussion of 'responsible learning' in Section 5 is very brief; expanding on this with concrete examples of how ACE enables selective unlearning or responsible context evolution would strengthen the argument.",
      "In Table 1, the 'Average' column seems to be an average across the four metrics (TGC/SGC for Test-Normal/Test-Challenge). Clarifying this explicitly in the caption would be helpful."
    ],
    "questions_for_authors": [
      "Could the authors provide more details on the specific types of 'natural execution feedback' leveraged by ACE for adaptation without labeled supervision, and how its robustness compares to cases with direct ground-truth labels?",
      "For the 'grow-and-refine' mechanism, what specific thresholds or criteria are used for proactive vs. lazy de-duplication and how do these choices impact performance and efficiency for different task types?",
      "Given that ACE surpasses IBM-CUGA on the test-challenge split (Table 1), can the authors elaborate on the characteristics of this harder split that allow ACE to shine, and perhaps delve into error analysis differences between the two agents?",
      "In the cost and speed analysis (Table 4), could the authors offer a more granular breakdown of what contributes to 'token dollar cost' (e.g., generation vs. ingestion, LLM API calls vs. other operations)?"
    ]
  },
  "detailed_feedback": {
    "comments_for_authors": "This is an exceptionally strong paper that presents a highly innovative and impactful framework for Agentic Context Engineering (ACE). The authors successfully identify and address crucial limitations of existing context adaptation methods, namely 'brevity bias' and 'context collapse,' through a well-designed modular architecture and novel principles like incremental delta updates and grow-and-refine. The empirical evaluation is comprehensive, demonstrating consistent and significant performance gains across diverse benchmarks (AppWorld, FINER, Formula) over strong baselines like ICL, MIPROv2, GEPA, and Dynamic Cheatsheet. The efficiency benefits, particularly the drastic reduction in adaptation latency and costs, are a major selling point, making ACE highly practical for real-world LLM deployments. The ability to adapt effectively without labeled supervision, relying instead on execution feedback, is a notable strength that pushes the boundaries of self-improving AI systems. The paper is extremely well-written, clear, and logically organized, with high-quality figures and tables that effectively convey complex information. The detailed explanation of the methodology, ablation studies, and robust comparison to prior work reflect thorough research. My main concern relates to the explicit reliance on 'reliable feedback signals' even when operating without labels, which could be elaborated more to clarify the framework's true autonomous learning capabilities in diverse environments. Overall, this paper makes a substantial contribution to the field of AI agents and LLM engineering and is well-positioned for acceptance.",
    "confidential_comments_to_editor": "This paper is a strong candidate for acceptance. The work is technically sound, highly original, and presents compelling empirical results across multiple benchmarks, including outperforming a production-level GPT-4.1 agent in certain aspects with a smaller open-source model. The authors effectively tackle known issues in context adaptation, offering a scalable and efficient solution. The reproducibility aspects are well-addressed with detailed methodology and prompt examples. I recommend a 'strong_accept' or 'accept' after considering if minor clarifications regarding feedback dependency and ethical considerations can be addressed."
  },
  "recommendation": {
    "decision": "strong_accept",
    "confidence": "high",
    "justification": "The paper presents a highly original and robust framework (ACE) that effectively solves critical problems (brevity bias, context collapse) in LLM context adaptation. It demonstrates superior performance, efficiency, and scalability across diverse benchmarks, including matching and surpassing state-of-the-art proprietary agents with smaller models. The methodology is sound, results are compelling, and the writing is excellent, making a significant contribution to the field.",
    "conditions_for_acceptance": null
  }
}