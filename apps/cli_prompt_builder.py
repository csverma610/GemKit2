mport sys
import json
from pydantic import BaseModel, Field, ValidationError

class GeminiClient:
    """
    A placeholder client for the Gemini API that encapsulates
    connection details and request logic.
    """
    def __init__(self):
        # Configuration details typically stored here
        self.model_name = "gemini-2.5-flash-preview-05-20"
        self.apiKey = "" # Assumed to be provided by the environment or client config
        self.apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model_name}:generateContent?key={self.apiKey}"
        # Setting placeholders for retry logic if it were implemented here
        self.MAX_RETRIES = 5
        self.INITIAL_DELAY = 1

    def generate_structured_content(self, user_query: str, system_instruction: str, response_schema: dict) -> str:
        """
        Sends a request to the Gemini API configured for structured JSON output.
        
        Args:
            user_query: The specific task prompt.
            system_instruction: The persona and role constraint for the LLM.
            response_schema: The Pydantic schema converted to a JSON dictionary.
            
        Returns:
            The raw JSON string generated by the LLM.
        """
        # 1. Construct the API Payload
        payload = {
            "contents": [{ "parts": [{ "text": user_query }] }],
            "generationConfig": {
                "responseMimeType": "application/json",
                "responseSchema": response_schema
            },
            "systemInstruction": {
                "parts": [{ "text": system_instruction }]
            }
        }
        
        # 2. Simulate/Perform the API call (The actual implementation
        #    would include requests.post and fetch_with_retry logic here)
        
        # --- MOCK API RESPONSE START (Replace with actual network code in a real client) ---
        # This mocks the LLM successfully returning structured JSON content.
        mock_response_text = json.dumps({
            "system_prompt": "You are a prompt engineering specialist focused on optimizing instruction clarity and role-setting.",
            "user_prompt": f"Analyze the following specification: '{user_query[:50]}...' and deliver the best possible system and user prompt for this task."
        }, indent=2)

        # In a real scenario, you would parse the successful response JSON
        # and extract the text from 'candidates[0].content.parts[0].text'.
        return mock_response_text
        # --- MOCK API RESPONSE END ---

# --- Pydantic Schema for Structured Output ---
# This defines the exact structure and types the LLM MUST conform to.
class GeneratedPrompts(BaseModel):
    """Schema for the system and user prompts generated by the LLM."""
    system_prompt: str = Field(..., description="The high-level instruction that sets the tone, role, and constraints for the LLM.")
    user_prompt: str = Field(..., description="The specific task or query provided by the end-user.")

def build_prompts(specs: str):
    """
    Generates a system and user prompt based on user specifications using the 
    GeminiClient and enforcing a Pydantic structure for the output.
    """
    # 1. Initialize the client
    llm = GeminiClient()

    # 2. Define the core inputs
    user_query = f"Based on the following functional specifications, generate a concise system prompt and a detailed, engaging user prompt:\n\nSPECIFICATIONS:\n{specs}"
    
    system_instruction = "You are an expert prompt engineer. Your task is to analyze user-provided specifications and generate two distinct prompts: a high-level system prompt for the AI's configuration, and a specific user-facing prompt for the task execution. You MUST output a valid JSON object conforming to the provided schema."

    # 3. Prepare the Pydantic schema for the API request
    # .model_json_schema() creates the necessary JSON schema object for the API.
    response_schema = GeneratedPrompts.model_json_schema()
    
    print(f"--- Preparing Structured Request ---")
    print(f"Specifications: {specs[:100]}...") # Show truncated specs

    try:
        # 4. Use the client to generate content with the structured schema
        raw_json_text = llm.generate_structured_content(
            user_query=user_query,
            system_instruction=system_instruction,
            response_schema=response_schema
        )

        if not raw_json_text:
            print("Error: Client did not return structured JSON text.")
            return

        # 5. Parse the JSON string into a Python dictionary
        response_dict = json.loads(raw_json_text)

        # 6. Validate the dictionary against the Pydantic model
        # Pydantic ensures the structure is correct before we use the data.
        validated_prompts = GeneratedPrompts.model_validate(response_dict)

        print("\n--- Successfully Generated and Validated Prompts (Pydantic Output) ---")
        print(f"System Prompt:\n{validated_prompts.system_prompt}\n")
        print(f"User Prompt:\n{validated_prompts.user_prompt}")
        
    except (json.JSONDecodeError, ValidationError) as e:
        # Catch cases where the LLM or client returns invalid JSON or non-conforming structure
        print(f"\nCritical Error during prompt generation or validation: {e}")
        print("Received Text for Debugging:\n", raw_json_text)
    except Exception as e:
        # Catch other potential errors from the client
        print(f"\nAn unexpected error occurred: {e}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python prompt_generator.py \"<specifications>\"")
        print("Example: python prompt_generator.py \"A tool that takes a topic and generates a two-part conversation between a historian and a student.\"")
        sys.exit(1)

    specs = sys.argv[1]
    
    # We call the function, which internally uses the Pydantic schema for output
    build_prompts(specs)

